{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707723fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, AutoProcessor, AutoModelForVision2Seq\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Could not open {video_path}\")\n",
    "\n",
    "    prev_sec = -1\n",
    "    saved = 0\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # get current position in milliseconds\n",
    "        ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        sec = int(ms // 1000)\n",
    "\n",
    "        # save first frame of each new second\n",
    "        if sec > prev_sec:\n",
    "            prev_sec = sec\n",
    "            fname = os.path.join(output_folder, f\"frame_{sec:03d}s.jpg\")\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved} frames — one per second.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a298c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This workflow was not at all performant, and took 238 minutes to process a 26 second clip.\n",
    "'''bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=\"llava-hf/llava-1.5-7b-hf\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "def caption_frames(frames_dir=\"frames\", output_json=\"captions.json\"):\n",
    "    captions = {}\n",
    "    for fname in sorted(os.listdir(frames_dir)):\n",
    "        if not fname.lower().endswith(\".jpg\"):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(frames_dir, fname)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": img},\n",
    "                    {\"type\": \"text\",  \"text\": \"Describe the student's facial expression, gaze direction, and body posture in specific terms.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        out = pipe(text=messages, max_new_tokens=50, num_beams=3)[0]\n",
    "        caption = out[\"generated_text\"]\n",
    "        assistant_msgs = [m[\"content\"] for m in caption if m.get(\"role\")==\"assistant\"]\n",
    "\n",
    "        captions[fname] = assistant_msgs\n",
    "        print(f\"{fname} → {assistant_msgs}\")\n",
    "\n",
    "    # 3️⃣ Save all captions\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(captions, f, indent=2)\n",
    "\n",
    "    return captions'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cf9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-500M-Instruct\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cuda\":\n",
    "    autocast_ctx = torch.cuda.amp.autocast(dtype=torch.float16)\n",
    "else:                           # CPU fallback → do nothing special\n",
    "    from contextlib import nullcontext\n",
    "    autocast_ctx = nullcontext()\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-500M-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    _attn_implementation= \"eager\",\n",
    ").to(DEVICE)\n",
    "def caption_frames(frames_dir=\"frames\", output_json=\"captions.json\"):\n",
    "    captions = {}\n",
    "    for fname in sorted(os.listdir(frames_dir)):\n",
    "        if not fname.lower().endswith(\".jpg\"):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(frames_dir, fname)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": img},\n",
    "                    {\"type\": \"text\",  \"text\": \"““Describe **all** observable facial muscle states, eye openness, head tilt, torso posture, and any hand/arm positions. Avoid repeating defaults—if a cue isn’t present, say ‘none.’ For each category, use concrete, frame-specific language (e.g., ‘left eyebrow slightly raised,’ ‘right eye half-closed,’ ‘chin tucked,’ ‘leaning forward on elbows,’ ‘right hand supporting chin’).”””\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = processor(text=prompt, images=[img], return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v\n",
    "                  for k, v in inputs.items()}\n",
    "        with torch.amp.autocast(DEVICE):          # fp16, saves VRAM\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "        generated_texts = processor.batch_decode(\n",
    "            generated_ids,\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        raw = generated_texts[0]\n",
    "        if \"Assistant: \" in raw:\n",
    "            assistant_reply = raw.split(\"Assistant: \", 1)[1].strip()\n",
    "        else:\n",
    "            assistant_reply = raw\n",
    "        captions[fname] = assistant_reply\n",
    "        print(f\"{fname} : {assistant_reply}\")\n",
    "    \n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(captions, f, indent=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_analysis(caption_file, model_id=\"google/gemma-3-1b-it\"):\n",
    "    with open(caption_file, 'r') as f:\n",
    "        captions = json.load(f)\n",
    "    text = \"\\n\".join([f\"{k}: {v}\" for k, v in captions.items()])\n",
    "    prompt = f\"\"\"You are analyzing student behavior from video frames.\n",
    "Here are the frame-wise captions:\n",
    "{text}\n",
    "\n",
    "Is the student focused or distracted? Justify your answer in one paragraph.\n",
    "\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    #pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    gen_ids = outputs[0, input_ids.shape[-1]:]\n",
    "    result = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "    print(\"\\nLLM Analysis Result:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(video_path):\n",
    "    extract_frames(video_path, \"frames\")\n",
    "    caption_frames(\"frames\", \"captions.json\")\n",
    "    run_llm_analysis(\"captions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d49917",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline(\"testingVideo.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-Finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
